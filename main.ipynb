{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises-MRI-segmentation\n",
    "\n",
    "Coding exercises for appying to the position at the Paris Brain Institute. The base code is taken from the following tutorial: https://colab.research.google.com/github/fepegar/torchio-notebooks/blob/main/notebooks/TorchIO_MONAI_PyTorch_Lightning.ipynb#scrollTo=QixbF3koO99H. \n",
    "\n",
    "TODO: write an introduction to the problem, the type of data that is going to be used, the number of labels, training strategy, etc...\n",
    "\n",
    "## Original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import monai\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import torchio as tio\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "plt.rcParams['figure.figsize'] = 12, 8\n",
    "monai.utils.set_determinism()\n",
    "\n",
    "print('Last run on', time.ctime())\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalDecathlonDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, task, google_id, batch_size, train_val_ratio):\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.google_id = google_id\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_dir = Path(task)\n",
    "        self.train_val_ratio = train_val_ratio\n",
    "        self.subjects = None\n",
    "        self.test_subjects = None\n",
    "        self.preprocess = None\n",
    "        self.transform = None\n",
    "        self.train_set = None\n",
    "        self.val_set = None\n",
    "        self.test_set = None\n",
    "    \n",
    "    def get_max_shape(self, subjects):\n",
    "        import numpy as np\n",
    "        dataset = tio.SubjectsDataset(subjects)\n",
    "        shapes = np.array([s.spatial_shape for s in dataset])\n",
    "        return shapes.max(axis=0)\n",
    "    \n",
    "    def download_data(self):\n",
    "        if not self.dataset_dir.is_dir():\n",
    "            url = f'https://drive.google.com/uc?id={self.google_id}'\n",
    "            output = f'{self.task}.tar'\n",
    "            gdown.download(url, output, quiet=False)\n",
    "            !tar xf {output}\n",
    "\n",
    "        def get_niis(d):\n",
    "            return sorted(p for p in d.glob('*.nii*') if not p.name.startswith('.'))\n",
    "\n",
    "        image_training_paths = get_niis(self.dataset_dir / 'imagesTr')\n",
    "        label_training_paths = get_niis(self.dataset_dir / 'labelsTr')\n",
    "        image_test_paths = get_niis(self.dataset_dir / 'imagesTs')\n",
    "        return image_training_paths, label_training_paths, image_test_paths\n",
    "\n",
    "    def prepare_data(self):\n",
    "        image_training_paths, label_training_paths, image_test_paths = self.download_data()\n",
    "\n",
    "        self.subjects = []\n",
    "        for image_path, label_path in zip(image_training_paths, label_training_paths):\n",
    "            # 'image' and 'label' are arbitrary names for the images\n",
    "            subject = tio.Subject(\n",
    "                image=tio.ScalarImage(image_path),\n",
    "                label=tio.LabelMap(label_path)\n",
    "            )\n",
    "            self.subjects.append(subject)\n",
    "        \n",
    "        self.test_subjects = []\n",
    "        for image_path in image_test_paths:\n",
    "            subject = tio.Subject(image=tio.ScalarImage(image_path))\n",
    "            self.test_subjects.append(subject)\n",
    "    \n",
    "    def get_preprocessing_transform(self):\n",
    "        preprocess = tio.Compose([\n",
    "            tio.RescaleIntensity((-1, 1)),\n",
    "            tio.CropOrPad(self.get_max_shape(self.subjects + self.test_subjects)),\n",
    "            tio.EnsureShapeMultiple(8),  # for the U-Net\n",
    "            tio.OneHot(),\n",
    "        ])\n",
    "        return preprocess\n",
    "    \n",
    "    def get_augmentation_transform(self):\n",
    "        augment = tio.Compose([\n",
    "            tio.RandomAffine(),\n",
    "            tio.RandomGamma(p=0.5),\n",
    "            tio.RandomNoise(p=0.5),\n",
    "            tio.RandomMotion(p=0.1),\n",
    "            tio.RandomBiasField(p=0.25),\n",
    "        ])\n",
    "        return augment\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        num_subjects = len(self.subjects)\n",
    "        num_train_subjects = int(round(num_subjects * self.train_val_ratio))\n",
    "        num_val_subjects = num_subjects - num_train_subjects\n",
    "        splits = num_train_subjects, num_val_subjects\n",
    "        train_subjects, val_subjects = random_split(self.subjects, splits)\n",
    "\n",
    "        self.preprocess = self.get_preprocessing_transform()\n",
    "        augment = self.get_augmentation_transform()\n",
    "        self.transform = tio.Compose([self.preprocess, augment])\n",
    "\n",
    "        self.train_set = tio.SubjectsDataset(train_subjects, transform=self.transform)\n",
    "        self.val_set = tio.SubjectsDataset(val_subjects, transform=self.preprocess)\n",
    "        self.test_set = tio.SubjectsDataset(self.test_subjects, transform=self.preprocess)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MedicalDecathlonDataModule(\n",
    "    task='Task04_Hippocampus',\n",
    "    google_id='1RzPB1_bqzQhlWvU-YGvZzhx2omcDh38C',\n",
    "    batch_size=16,\n",
    "    train_val_ratio=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.prepare_data()\n",
    "data.setup()\n",
    "print('Training:  ', len(data.train_set))\n",
    "print('Validation: ', len(data.val_set))\n",
    "print('Test:      ', len(data.test_set))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, net, criterion, learning_rate, optimizer_class):\n",
    "        super().__init__()\n",
    "        self.lr = learning_rate\n",
    "        self.net = net\n",
    "        self.criterion = criterion\n",
    "        self.optimizer_class = optimizer_class\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer_class(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def prepare_batch(self, batch):\n",
    "        return batch['image'][tio.DATA], batch['label'][tio.DATA]\n",
    "    \n",
    "    def infer_batch(self, batch):\n",
    "        x, y = self.prepare_batch(batch)\n",
    "        y_hat = self.net(x)\n",
    "        return y_hat, y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat, y = self.infer_batch(batch)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat, y = self.infer_batch(batch)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net model from monai\n",
    "unet = monai.networks.nets.UNet(\n",
    "    dimensions=3,\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    channels=(8, 16, 32, 64),\n",
    "    strides=(2, 2, 2),\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    net=unet,\n",
    "    criterion=monai.losses.DiceCELoss(softmax=True),\n",
    "    learning_rate=1e-2,\n",
    "    optimizer_class=torch.optim.AdamW,\n",
    ")\n",
    "\n",
    "early_stopping = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    # precision=16,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "trainer.logger._default_hp_metric = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print('Training started at', start)\n",
    "trainer.fit(model=model, datamodule=data)\n",
    "print('Training duration:', datetime.now() - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "- Write a training code for a similar training as in the tutorial, but without the\n",
    "pytorch_lightning library.\n",
    "- Make one script with a command line for training.\n",
    "- In the training loop use the automatic mixed precision from Pytorch (with autocast and\n",
    "GradScaler) in order to train with FP16 precision instead of the default FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra imports \n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# hyperparameters\n",
    "config = {\n",
    "    'task': 'Task04_Hippocampus',\n",
    "    'google_id': '1RzPB1_bqzQhlWvU-YGvZzhx2omcDh38C',\n",
    "    'batch_size': 16,\n",
    "    'train_val_ratio': 0.8,\n",
    "\n",
    "    'epochs': 100,\n",
    "    'lr': 1e-2,\n",
    "    'early_stopping': 10, # -1 to disable, else insert patience\n",
    "\n",
    "    'best_models_dir': 'best_models',\n",
    "}\n",
    "\n",
    "assert config[\"early_stopping\"] == -1 or config[\"early_stopping\"] > 0, \"early_stopping must be -1 or > 0\"\n",
    "assert config[\"train_val_ratio\"] > 0 and config[\"train_val_ratio\"] < 1, \"train_val_ratio must be > 0 and < 1\"\n",
    "assert config[\"batch_size\"] > 0, \"batch_size must be > 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data download & preparation\n",
    "data = MedicalDecathlonDataModule(\n",
    "    task=config['task'],\n",
    "    google_id=config['google_id'],\n",
    "    batch_size=config['batch_size'],\n",
    "    train_val_ratio=config['train_val_ratio'],\n",
    ")\n",
    "\n",
    "data.prepare_data()\n",
    "data.setup()\n",
    "\n",
    "train_data_loader = data.train_dataloader()\n",
    "val_data_loader = data.val_dataloader()\n",
    "test_data_loader = data.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a training example\n",
    "batch = next(iter(train_data_loader))\n",
    "\n",
    "batch_image = batch['image']['data']\n",
    "batch_label = batch['label']['data']\n",
    "\n",
    "print(f'The shape of the data is {batch_image.shape}')\n",
    "\n",
    "# plot a slice and the corresponding label\n",
    "slice_idx = 30\n",
    "\n",
    "plt.figure('image', (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('image')\n",
    "plt.imshow(batch_image[0, 0, :, :, slice_idx], cmap='gray')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('label')\n",
    "plt.imshow(batch_label[0, 0, :, :, slice_idx], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net model from monai\n",
    "unet = monai.networks.nets.UNet(\n",
    "    dimensions=3,\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    channels=(8, 16, 32, 64),\n",
    "    strides=(2, 2, 2),\n",
    ")\n",
    "\n",
    "model = unet\n",
    "criterion=monai.losses.DiceCELoss(softmax=True)\n",
    "optimizer=torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for early stopping \n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for batch in train_data_loader:\n",
    "        x, y = batch['image']['data'].to(device), batch['label']['data'].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validation loop\n",
    "    model.eval()\n",
    "    for batch in val_data_loader:\n",
    "        x, y = batch['image']['data'].to(device), batch['label']['data'].to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        val_loss = criterion(logits, y)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{config[\"epochs\"]}, train loss: {loss.item():.4f}, val loss: {val_loss.item():.4f}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), os.path.join(config['best_models_dir'], f'best_model.pth'))\n",
    "\n",
    "    elif config['early_stopping'] != -1:\n",
    "        patience_counter += 1\n",
    "        if patience_counter == config['early_stopping']:\n",
    "            print('Training stopped due to early stopping')\n",
    "            break\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(config['best_models_dir'], f'last_model.pth'))\n",
    "\n",
    "# test loop\n",
    "pass\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ParisBI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
